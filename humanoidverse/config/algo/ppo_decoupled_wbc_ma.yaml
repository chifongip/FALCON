# @package _global_

algo:
  _target_: humanoidverse.agents.decouple.ppo_decoupled_wbc_ma.PPOMultiActorCritic
  _recursive_: False
  config:
    num_learning_epochs: 5
    num_mini_batches: 4
    clip_param: 0.2
    gamma: 0.99
    lam: 0.95
    value_loss_coef: 1.0
    entropy_coef: 0.01
    actor_learning_rate: 1.e-4 # 5e-4 # 1.e-3
    critic_learning_rate: 1.e-4 # 5e-4 # 1.e-3
    weight_decay: 1.e-2
    max_grad_norm: 1.0
    use_clipped_value_loss: True
    schedule: "adaptive"
    desired_kl: 0.01

    num_steps_per_env: 24
    save_interval: 1000

    load_optimizer: True

    init_noise_std: {
      lower_body: 0.8, 
      upper_body: 0.6, # 0.6 for residual, 0.6 for non-residual
    }

    num_learning_iterations: 10000
    init_at_random_ep_len: True
    eval_callbacks: null

    log_all_action_std: True

    module_dict:
      actor_lower_body:
        input_dim: [actor_obs]
        history_length: ${obs.history_length}
        output_dim: [robot_action_dim]
        layer_config:
          type: MLP
          hidden_dims: [512, 256, 128]
          activation: ELU
        # min_noise_std: 0.06
        # min_mean_noise_std: 0.20
      actor_upper_body:
        input_dim: [actor_obs]
        history_length: ${obs.history_length}
        output_dim: [robot_action_dim]
        layer_config:
          type: MLP
          hidden_dims: [512, 256, 128]
          activation: ELU
        # min_noise_std: 0.12
        # min_mean_noise_std: 0.18
      critic: 
        type: MLP
        input_dim: [critic_obs]
        history_length: ${obs.history_length}
        output_dim: [1]
        layer_config:
          type: MLP
          hidden_dims: [512, 256, 128]
          activation: ELU